{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Pytorch Baseline - Train\n",
    "\n",
    "**Notes**\n",
    "- Do not forget to enable the GPU (TPU) for training\n",
    "- You have to add `kaggle_l5kit` as utility script\n",
    "- Parts of the code below is from the [official example](https://github.com/lyft/l5kit/blob/master/examples/agent_motion_prediction/agent_motion_prediction.ipynb)\n",
    "- [Baseline inference notebook](https://www.kaggle.com/pestipeti/pytorch-baseline-inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.resnet import resnet18,resnet50,resnet101\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "from torch import functional as F\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_INPUT = \"/media/ubuntu/Data/project/lyft/lyft-motion-prediction-autonomous-vehicles/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'format_version': 4,\n",
    "    'model_params': {\n",
    "        'model_architecture': 'resnet18',\n",
    "        \n",
    "        'history_num_frames': 10,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        \n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1\n",
    "    },\n",
    "    \n",
    "    'raster_params': {\n",
    "        'raster_size': [1, 1],\n",
    "        'pixel_size': [0.5, 0.5],\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': 'py_semantic',\n",
    "        'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "        'dataset_meta_key': 'meta.json',\n",
    "        'filter_agents_threshold': 0.5\n",
    "    },\n",
    "    \n",
    "    'train_data_loader': {\n",
    "        'key': 'scenes/train.zarr',\n",
    "        'batch_size': 32,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "    \n",
    "    'val_data_loader': {\n",
    "        'key': 'scenes/validate.zarr',\n",
    "        'batch_size': 32,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "    \n",
    "    'test_data_loader': {\n",
    "        'key': 'scenes/test.zarr',\n",
    "        'batch_size': 32,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "    \n",
    "    'train_params': {\n",
    "        'checkpoint_every_n_steps': 5000,\n",
    "        'max_num_steps': 85000,\n",
    "        'eval_every_n_steps': 500\n",
    "\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\n",
    "dm = LocalDataManager(None)\n",
    "VALIDATION = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   16265    |  4039527   | 320124624  |      112.19     |        248.36        |        79.25         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "22496709\n"
     ]
    }
   ],
   "source": [
    "# ===== INIT DATASET\n",
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "\n",
    "# Rasterizer\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "\n",
    "# Train dataset/dataloader\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              shuffle=train_cfg[\"shuffle\"],\n",
    "                              batch_size=train_cfg[\"batch_size\"])\n",
    "                              #num_workers=train_cfg[\"num_workers\"])\n",
    "\n",
    "print(train_dataset)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   16220    |  4030296   | 312617887  |      111.97     |        248.48        |        77.57         |        24.85         |        10.00        |\n",
      "+------------+------------+------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "21624612\n"
     ]
    }
   ],
   "source": [
    "# ===== INIT  VAL DATASET\n",
    "val_cfg = cfg[\"val_data_loader\"]\n",
    "\n",
    "# Rasterizer\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "\n",
    "# Train dataset/dataloader\n",
    "val_zarr = ChunkedDataset(dm.require(val_cfg[\"key\"])).open()\n",
    "val_dataset = AgentDataset(cfg, val_zarr, rasterizer)\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                              shuffle=val_cfg[\"shuffle\"],\n",
    "                              batch_size=val_cfg[\"batch_size\"])\n",
    "                              #num_workers=train_cfg[\"num_workers\"])\n",
    "\n",
    "print(val_dataset)\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_it = iter(train_dataloader)\n",
    "data = next(tr_it)\n",
    "history_positions = data['target_positions']\n",
    "history_positions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM_LyftModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super(EncoderLSTM_LyftModel, self).__init__()\n",
    "        \n",
    "        self.input_sz  = 2\n",
    "        self.hidden_sz = 128\n",
    "        self.num_layer = 1\n",
    "        self.sequence_length = 11        \n",
    "        \n",
    "        self.Encoder_lstm = nn.LSTM(self.input_sz,self.hidden_sz,self.num_layer,batch_first=True)\n",
    "       \n",
    "    def forward(self,inputs):\n",
    "        \n",
    "        output,hidden_state = self.Encoder_lstm(inputs)\n",
    "        \n",
    "        return output,hidden_state\n",
    "    \n",
    "class DecoderLSTM_LyftModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(DecoderLSTM_LyftModel, self).__init__()\n",
    "        \n",
    "        self.input_sz  = 128 #(2000 from fcn_en_output reshape to 50*40)\n",
    "        self.hidden_sz = 128\n",
    "        self.hidden_sz_en = 128\n",
    "        self.num_layer = 1\n",
    "        self.input_sz_en = 2\n",
    "        self.sequence_len_en = 11\n",
    "        self.sequence_len_de = 1\n",
    "        \n",
    "        self.interlayer = 64\n",
    "\n",
    "\n",
    "        num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "        \n",
    "        self.encoderLSTM = EncoderLSTM_LyftModel (cfg)\n",
    "\n",
    "        \n",
    "        self.Decoder_lstm = nn.LSTM( self.input_sz,self.hidden_sz,self.num_layer,batch_first=True)\n",
    "\n",
    "        \n",
    "        self.fcn_en_state_dec_state= nn.Sequential(nn.Linear(in_features=self.hidden_sz_en, out_features=self.interlayer),\n",
    "                            nn.Linear(in_features=self.interlayer, out_features=num_targets))\n",
    "        \n",
    "\n",
    "    def forward(self,inputs):        \n",
    "        \n",
    "        _,hidden_state = self.encoderLSTM(inputs)\n",
    "        \n",
    "        #inout_to_dec = torch.ones(inputs.shape[0],self.sequence_len_de,self.input_sz).to(device)\n",
    "\n",
    "        inout_to_dec = hidden_state[0].reshape(inputs.shape[0],self.sequence_len_de,self.input_sz).to(device)\n",
    "        \n",
    "        #for i in range(cfg[\"model_params\"][\"future_num_frames\"]+1):\n",
    "        inout_to_dec,hidden_state   = self.Decoder_lstm(inout_to_dec,(hidden_state[0],hidden_state[1]) )          \n",
    "        \n",
    "        fc_out = self.fcn_en_state_dec_state (inout_to_dec.squeeze(dim=0))\n",
    "        \n",
    "        return fc_out.reshape(inputs.shape[0],cfg[\"model_params\"][\"future_num_frames\"],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== INIT MODEL\n",
    "model = DecoderLSTM_LyftModel(cfg)\n",
    "model.to(device)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1e-2,momentum=0.9)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=75000,gamma=0.1)\n",
    "#lr_scheduler = CyclicLR(optimizer, base_lr=1e-2, max_lr=1e-1,cycle_momentum = True)\n",
    "# Later we have to filter the invalid steps.\n",
    "criterion = nn.MSELoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderLSTM_LyftModel(\n",
       "  (encoderLSTM): EncoderLSTM_LyftModel(\n",
       "    (Encoder_lstm): LSTM(2, 128, batch_first=True)\n",
       "  )\n",
       "  (Decoder_lstm): LSTM(128, 128, batch_first=True)\n",
       "  (fcn_en_state_dec_state): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=100, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint = torch.load('/media/ubuntu/Data/project/lyft/l5kit-1.0.6/examples/agent_motion_prediction/model/model_state_last_40k_18_false.pth'))\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#loss=checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " TrainLoss: 2.3924 ValLoss: 3.3134 TrainMeanLoss: 3.373432037242677 ValMeanLoss: 3.6309106161109126: 100%|██████████| 85000/85000 [38:11:35<00:00,  1.62s/it]     \n"
     ]
    }
   ],
   "source": [
    "# ==== TRAIN LOOP\n",
    "tr_it = iter(train_dataloader)\n",
    "vl_it = iter(val_dataloader)\n",
    "\n",
    "progress_bar = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\n",
    "losses_train = []\n",
    "losses_mean_train = []\n",
    "losses_val = []\n",
    "losses_mean_val = []\n",
    "\n",
    "for itr in progress_bar:\n",
    "    try:\n",
    "        data = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(train_dataloader)\n",
    "        data = next(tr_it)\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    # Forward pass\n",
    "    history_positions = data['history_positions'].to(device)\n",
    "    history_availabilities = data['history_availabilities'].to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "    targets_position = data[\"target_positions\"].to(device)\n",
    "\n",
    "    outputs = model(history_positions)\n",
    "\n",
    "    loss = criterion(outputs,targets_position)\n",
    "    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
    "    loss = loss * target_availabilities\n",
    "    loss = loss.mean()\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    losses_train.append(loss.item())\n",
    "    losses_mean_train.append(np.mean(losses_train))\n",
    "    \n",
    "    # Validation\n",
    "    if VALIDATION :#& ( cfg[\"train_params\"][\"max_num_steps\"] % cfg[\"train_params\"][\"eval_every_n_steps\"] ==0 ):\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                val_data = next(vl_it)\n",
    "            except StopIteration:\n",
    "                vl_it = iter(val_dataloader)\n",
    "                val_data = next(vl_it)\n",
    "\n",
    "            model.eval()\n",
    "            # Forward pass\n",
    "            target_availabilities_val = val_data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "            targets_val = val_data[\"target_positions\"].to(device)\n",
    "            history_positions_val = val_data['history_positions'].to(device)\n",
    "            history_availabilities_val = data['history_availabilities'].to(device)\n",
    "\n",
    "            outputs_val = model(history_positions_val)\n",
    "                    \n",
    "            loss_v = criterion(outputs_val,targets_val)\n",
    "            # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
    "            loss_v = loss_v * target_availabilities_val\n",
    "            loss_v = loss_v.mean()\n",
    "\n",
    "            losses_val.append(loss_v.item())\n",
    "\n",
    "            losses_mean_val.append(np.mean(losses_val))\n",
    "\n",
    "\n",
    "        desc = f\" TrainLoss: {round(loss.item(), 4)} ValLoss: {round(loss_v.item(), 4)} TrainMeanLoss: {np.mean(losses_train)} ValMeanLoss: {np.mean(losses_val)}\" \n",
    "    else:\n",
    "        desc = f\" TrainLoss: {round(loss.item(), 4)}\"\n",
    "\n",
    "\n",
    "        #if len(losses_train)>0 and loss < min(losses_train):\n",
    "        #    print(f\"Loss improved from {min(losses_train)} to {loss}\")\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    progress_bar.set_description(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict' : model.state_dict(),\n",
    "            'optimizer_state_dict' : optimizer.state_dict(),\n",
    "            'loss' : loss },\n",
    "            '/media/ubuntu/Data/project/lyft/l5kit-1.0.6/examples/agent_motion_prediction/model/model_state_last_ENDC_real_lstm_10k.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning, you're running with a custom agents_mask\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f1b03402f10>\n"
     ]
    }
   ],
   "source": [
    "# ===== INIT DATASET\n",
    "test_cfg = cfg[\"test_data_loader\"]\n",
    "\n",
    "# Rasterizer\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "\n",
    "# Test dataset/dataloader\n",
    "test_zarr = ChunkedDataset(dm.require(test_cfg[\"key\"])).open()\n",
    "test_mask = np.load(f\"{DIR_INPUT}/scenes/mask.npz\")[\"arr_0\"]\n",
    "test_dataset = AgentDataset(cfg, test_zarr, rasterizer, agents_mask=test_mask)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             shuffle=test_cfg[\"shuffle\"],\n",
    "                             batch_size=test_cfg[\"batch_size\"],\n",
    "                             num_workers=test_cfg[\"num_workers\"])\n",
    "\n",
    "\n",
    "print(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2223/2223 [11:40<00:00,  3.17it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "future_coords_offsets_pd = []\n",
    "timestamps = []\n",
    "agent_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    dataiter = tqdm(test_dataloader)\n",
    "    \n",
    "    for data in dataiter:\n",
    "\n",
    "        history_positions = data['history_positions'].to(device)\n",
    "\n",
    "        outputs = model(history_positions)\n",
    "        \n",
    "        future_coords_offsets_pd.append(outputs.cpu().numpy().copy())\n",
    "        timestamps.append(data[\"timestamp\"].numpy().copy())\n",
    "        agent_ids.append(data[\"track_id\"].numpy().copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pred_csv('/media/ubuntu/Data/project/lyft/l5kit-1.0.6/examples/agent_motion_prediction/submission_ENDC_lstm_85k.csv',\n",
    "               timestamps=np.concatenate(timestamps),\n",
    "               track_ids=np.concatenate(agent_ids),\n",
    "               coords=np.concatenate(future_coords_offsets_pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write_pred_csv('/kaggle/working/submission.csv',\n",
    "               timestamps=np.concatenate(timestamps),\n",
    "               track_ids=np.concatenate(agent_ids),\n",
    "               coords=np.concatenate(future_coords_offsets_pd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
